name: SuperClaude MCP Migration Test Suite

on:
  push:
    branches: [ main, develop, release/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'full'
        type: choice
        options:
          - unit
          - integration
          - e2e
          - migration
          - performance
          - full
      environment:
        description: 'Environment to test against'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
          - local

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.9'
  
jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.test-matrix.outputs.matrix }}
      cache-key: ${{ steps.cache-key.outputs.key }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Generate test matrix
        id: test-matrix
        run: |
          if [ "${{ github.event.inputs.test_type }}" = "unit" ]; then
            echo "matrix={\"test_type\": [\"unit\"]}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_type }}" = "integration" ]; then
            echo "matrix={\"test_type\": [\"integration\"]}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_type }}" = "e2e" ]; then
            echo "matrix={\"test_type\": [\"e2e\"]}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_type }}" = "migration" ]; then
            echo "matrix={\"test_type\": [\"migration\"]}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_type }}" = "performance" ]; then
            echo "matrix={\"test_type\": [\"performance\", \"load\", \"stress\"]}" >> $GITHUB_OUTPUT
          else
            echo "matrix={\"test_type\": [\"unit\", \"integration\", \"e2e\", \"migration\", \"performance\"]}" >> $GITHUB_OUTPUT
          fi
          
      - name: Generate cache key
        id: cache-key
        run: |
          echo "key=test-deps-${{ runner.os }}-node${{ env.NODE_VERSION }}-${{ hashFiles('**/package-lock.json') }}" >> $GITHUB_OUTPUT

  unit-tests:
    needs: setup
    if: contains(fromJson(needs.setup.outputs.test-matrix).test_type, 'unit')
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: Tests/mcp_migration_suite/package-lock.json
          
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            node_modules
            MCP_Servers/node_modules
          key: ${{ needs.setup.outputs.cache-key }}
          restore-keys: |
            test-deps-${{ runner.os }}-node${{ env.NODE_VERSION }}-
            
      - name: Install dependencies
        working-directory: Tests/mcp_migration_suite
        run: |
          npm ci
          cd ../../MCP_Servers && npm ci
          
      - name: Build MCP servers
        working-directory: MCP_Servers
        run: npm run build
        
      - name: Start MCP servers
        working-directory: Tests/mcp_migration_suite
        run: npm run start:mcp-servers &
        
      - name: Wait for services
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:3001/health; do sleep 2; done'
          timeout 60 bash -c 'until curl -f http://localhost:3002/health; do sleep 2; done'
          timeout 60 bash -c 'until curl -f http://localhost:3003/health; do sleep 2; done'
          
      - name: Run unit tests
        working-directory: Tests/mcp_migration_suite
        run: npm run test:unit
        env:
          CI: true
          
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: unit-test-results
          path: |
            Tests/mcp_migration_suite/test-results/
            Tests/mcp_migration_suite/coverage/
          retention-days: 7

  integration-tests:
    needs: setup
    if: contains(fromJson(needs.setup.outputs.test-matrix).test_type, 'integration')
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      fail-fast: false
      matrix:
        bridge-component: [dispatcher, performance, context, integration]
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        working-directory: Tests/mcp_migration_suite
        run: |
          npm ci
          cd ../../MCP_Servers && npm ci
          cd ../../SuperClaude/Hooks && pip install -r requirements.txt
          
      - name: Start test environment
        working-directory: Tests/mcp_migration_suite
        run: npm run setup:test-env
        
      - name: Run integration tests
        working-directory: Tests/mcp_migration_suite
        run: npm run test:integration -- --testNamePattern="${{ matrix.bridge-component }}"
        env:
          CI: true
          TEST_COMPONENT: ${{ matrix.bridge-component }}
          
      - name: Cleanup test environment
        if: always()
        working-directory: Tests/mcp_migration_suite
        run: npm run teardown:test-env
        
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-results-${{ matrix.bridge-component }}
          path: Tests/mcp_migration_suite/test-results/
          retention-days: 7

  e2e-tests:
    needs: setup
    if: contains(fromJson(needs.setup.outputs.test-matrix).test_type, 'e2e')
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    strategy:
      fail-fast: false
      matrix:
        workflow: [analyze, build, improve, scan, review]
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        working-directory: Tests/mcp_migration_suite
        run: |
          npm ci
          cd ../../MCP_Servers && npm ci && npm run build
          cd ../../SuperClaude/Hooks && pip install -r requirements.txt
          
      - name: Start full test environment
        working-directory: Tests/mcp_migration_suite
        run: npm run setup:test-env
        
      - name: Run E2E workflow tests
        working-directory: Tests/mcp_migration_suite
        run: npm run test:e2e -- --testNamePattern="${{ matrix.workflow }}"
        env:
          CI: true
          TEST_WORKFLOW: ${{ matrix.workflow }}
          E2E_TIMEOUT: 300000
          
      - name: Capture screenshots on failure
        if: failure()
        run: |
          mkdir -p Screenshots/mcp_migration_suite/screenshots
          # Screenshots would be captured by Playwright if UI testing was involved
          
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-test-results-${{ matrix.workflow }}
          path: |
            Tests/mcp_migration_suite/test-results/
            Tests/mcp_migration_suite/screenshots/
          retention-days: 7

  migration-tests:
    needs: setup
    if: contains(fromJson(needs.setup.outputs.test-matrix).test_type, 'migration')
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        working-directory: Tests/mcp_migration_suite
        run: |
          npm ci
          cd ../../MCP_Servers && npm ci && npm run build
          
      - name: Setup legacy system mock
        run: |
          # Start mock legacy system
          cd Tests/mcp_migration_suite
          npm run start:legacy-mock &
          
      - name: Start new MCP system
        working-directory: Tests/mcp_migration_suite
        run: npm run setup:test-env
        
      - name: Run migration tests
        working-directory: Tests/mcp_migration_suite
        run: npm run test:migration
        env:
          CI: true
          LEGACY_SYSTEM_URL: http://localhost:3100
          NEW_SYSTEM_URL: http://localhost:3000
          
      - name: Generate migration report
        if: always()
        working-directory: Tests/mcp_migration_suite
        run: |
          npm run generate:migration-report
          
      - name: Upload migration results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: migration-test-results
          path: |
            Tests/mcp_migration_suite/test-results/
            Tests/mcp_migration_suite/migration-report.html
          retention-days: 14

  performance-tests:
    needs: setup
    if: contains(fromJson(needs.setup.outputs.test-matrix).test_type, 'performance')
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    strategy:
      fail-fast: false
      matrix:
        test-type: [performance, load, stress]
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Install dependencies
        working-directory: Tests/mcp_migration_suite
        run: |
          npm ci
          cd ../../MCP_Servers && npm ci && npm run build
          
      - name: Configure system for performance testing
        run: |
          # Increase system limits for load testing
          echo "fs.file-max = 100000" | sudo tee -a /etc/sysctl.conf
          ulimit -n 65536
          
      - name: Start test environment
        working-directory: Tests/mcp_migration_suite
        run: npm run setup:test-env
        
      - name: Run performance tests
        working-directory: Tests/mcp_migration_suite
        run: npm run test:${{ matrix.test-type }}
        env:
          CI: true
          PERFORMANCE_MODE: true
          
      - name: Generate performance report
        if: always()
        working-directory: Tests/mcp_migration_suite
        run: npm run generate:performance-report
        
      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-test-results-${{ matrix.test-type }}
          path: |
            Tests/mcp_migration_suite/test-results/
            Tests/mcp_migration_suite/performance-report.html
          retention-days: 14

  test-summary:
    needs: [unit-tests, integration-tests, e2e-tests, migration-tests, performance-tests]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v3
        with:
          path: test-artifacts/
          
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Install report generator
        run: |
          npm install -g junit-merge jest-html-reporters
          
      - name: Merge test results
        run: |
          mkdir -p merged-results
          find test-artifacts -name "junit.xml" -exec cp {} merged-results/ \;
          junit-merge -d merged-results -o merged-results/merged-junit.xml
          
      - name: Generate comprehensive report
        run: |
          cat > generate-report.js << 'EOF'
          const fs = require('fs');
          const path = require('path');
          
          // Aggregate all test results
          const testResults = {
            summary: {
              total_suites: 0,
              total_tests: 0,
              passed: 0,
              failed: 0,
              skipped: 0,
              success_rate: 0
            },
            performance: {
              unit_test_avg: 0,
              integration_test_avg: 0,
              e2e_test_avg: 0,
              performance_thresholds_met: false
            },
            migration: {
              feature_parity: 0,
              data_integrity: false,
              performance_regression: 0
            },
            recommendations: []
          };
          
          // Process results and generate recommendations
          // ... (implementation would parse actual test results)
          
          // Generate HTML report
          const html = `
          <!DOCTYPE html>
          <html>
          <head>
            <title>SuperClaude MCP Migration Test Report</title>
            <style>
              body { font-family: Arial, sans-serif; margin: 40px; }
              .header { background: #f5f5f5; padding: 20px; border-radius: 5px; }
              .summary { display: flex; gap: 20px; margin: 20px 0; }
              .metric { background: white; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }
              .success { color: green; } .failure { color: red; } .warning { color: orange; }
            </style>
          </head>
          <body>
            <div class="header">
              <h1>SuperClaude MCP Migration Test Report</h1>
              <p>Generated: ${new Date().toISOString()}</p>
              <p>Build: ${process.env.GITHUB_RUN_NUMBER || 'local'}</p>
            </div>
            
            <div class="summary">
              <div class="metric">
                <h3>Total Tests</h3>
                <div class="value">${testResults.summary.total_tests}</div>
              </div>
              <div class="metric">
                <h3>Success Rate</h3>
                <div class="value ${testResults.summary.success_rate > 90 ? 'success' : 'failure'}">
                  ${testResults.summary.success_rate.toFixed(1)}%
                </div>
              </div>
              <div class="metric">
                <h3>Migration Readiness</h3>
                <div class="value ${testResults.migration.feature_parity > 95 ? 'success' : 'warning'}">
                  ${testResults.migration.feature_parity.toFixed(1)}%
                </div>
              </div>
            </div>
            
            <!-- Additional sections would be added here -->
          </body>
          </html>
          `;
          
          fs.writeFileSync('comprehensive-test-report.html', html);
          EOF
          
          node generate-report.js
          
      - name: Upload comprehensive report
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-test-report
          path: |
            comprehensive-test-report.html
            merged-results/
          retention-days: 30
          
      - name: Post test summary to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            // Read test results and create summary comment
            const summary = `
            ## 🧪 SuperClaude MCP Migration Test Results
            
            **Overall Status:** ${{ job.status == 'success' && '✅ PASSED' || '❌ FAILED' }}
            
            ### Test Suite Results
            - Unit Tests: ${{ needs.unit-tests.result == 'success' && '✅' || '❌' }}
            - Integration Tests: ${{ needs.integration-tests.result == 'success' && '✅' || '❌' }}
            - E2E Tests: ${{ needs.e2e-tests.result == 'success' && '✅' || '❌' }}
            - Migration Tests: ${{ needs.migration-tests.result == 'success' && '✅' || '❌' }}
            - Performance Tests: ${{ needs.performance-tests.result == 'success' && '✅' || '❌' }}
            
            ### Migration Readiness
            - Feature Parity: Under analysis
            - Performance Impact: Under analysis
            - Data Integrity: Under analysis
            
            **Full Report:** [Download comprehensive report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  notify-teams:
    needs: [test-summary]
    if: always() && (failure() || success())
    runs-on: ubuntu-latest
    
    steps:
      - name: Notify on test completion
        uses: actions/github-script@v6
        with:
          script: |
            const status = '${{ job.status }}' === 'success' ? '✅ SUCCESS' : '❌ FAILURE';
            const webhook = process.env.TEAMS_WEBHOOK_URL;
            
            if (webhook) {
              const payload = {
                "@type": "MessageCard",
                "@context": "http://schema.org/extensions",
                "summary": `SuperClaude MCP Tests ${status}`,
                "themeColor": '${{ job.status }}' === 'success' ? "00FF00" : "FF0000",
                "sections": [{
                  "activityTitle": `SuperClaude MCP Migration Tests ${status}`,
                  "activitySubtitle": "Test suite execution completed",
                  "facts": [
                    {"name": "Repository", "value": "${{ github.repository }}"},
                    {"name": "Branch", "value": "${{ github.ref_name }}"},
                    {"name": "Run Number", "value": "${{ github.run_number }}"},
                    {"name": "Triggered By", "value": "${{ github.actor }}"}
                  ],
                  "markdown": true
                }],
                "potentialAction": [{
                  "@type": "OpenUri",
                  "name": "View Results",
                  "targets": [{
                    "os": "default",
                    "uri": "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }]
                }]
              };
              
              await fetch(webhook, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
              });
            }
        env:
          TEAMS_WEBHOOK_URL: ${{ secrets.TEAMS_WEBHOOK_URL }}

  deploy-staging:
    needs: [test-summary]
    if: github.ref == 'refs/heads/develop' && success()
    runs-on: ubuntu-latest
    environment: staging
    
    steps:
      - name: Deploy to staging
        run: |
          echo "Deploying SuperClaude MCP system to staging environment"
          # Deployment logic would go here
          
      - name: Run smoke tests
        run: |
          echo "Running post-deployment smoke tests"
          # Smoke test logic would go here